{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ded573",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` (you should of course delete `raise NotImplementedError()` which is there only as reminder), while not modifying the other cells (but you should run them to check the output you obtain). Please also fill in your group number, your full names, and your VU IDs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33baa447",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "STUDENT_FIRST_NAME = \"Thu Trang\" # e.g. \"John\" (no \"J\", no \"J.\", no \"John S.M.\")\n",
    "STUDENT_LAST_NAME = \"Luu\"  # e.g. \"Smith\"\n",
    "VU_ID_NUM = \"2695303\"          # e.g. \"2789012\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43643e2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05173e1c-842e-4ad3-875a-b2ba4a6fa125",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed01a5230e7cd2ba41ee28330c25c767",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Assignment 3\n",
    "The goal of this assignment is to make you familiar with feature engineering, statistical data analysis and machine learning in Python.\n",
    "\n",
    "This assignment is divided into 4 exercises, which are worth different amount of points (total is 100 points). We'll cover 1) Statistical Data Analysis, 2) Feature Engineering, 3) Classification, and 4) Regression. Similar to last assignment, **the input and desired output are described either in the text or in the provided function docstrings.**\n",
    "\n",
    "We assume that the folder that you work in is the one obtained by unzipping the given ``assignment03.zip`` file and thus has the following structure.\n",
    "\n",
    "\n",
    "```\n",
    "assignment03.ipynb\n",
    "data/\n",
    "    movielens/...\n",
    "    hue/...\n",
    "    spam/...\n",
    "```\n",
    "\n",
    "## Important remarks\n",
    "- **Working together**: You are meant to work individually on the first three assignments. You can, of course, brainstorm ideas and discuss issues with your fellow students, but you are required to write your solutions individually. \n",
    "- **Plagiarism**: All your code will be automatically scanned for plagiarism. Furthermore, using the internet as a passive resource is allowed. This means that you can search for help there and partially copy code, as long as you explicitly acknowledge inside your Jupyter notebook which parts have been copied and from where. \n",
    "- **Performance**: You should optimize the computational performance of your functions. Specifically, when grading the assignments, we at times set a hard limit for each cell execution as specified in the exercise description. Function calls that take longer than that time threshold will not be awarded any points.\n",
    "- **Code styling**: Your implementation will not be checked for style. However, we do encourage you to practice good code styling. See, for example, https://docs.python-guide.org/writing/style/.\n",
    "- **Chronological run**: All outputs should be repeatable by doing one full “chronological” run of the notebook without any manual changes to code blocks, including parameters. (try it yourself by clicking ``Kernel -> Restart and run all``, which should give the result as handed in).\n",
    "- **Handing in**: Hand in the .ipynb file of your notebook on ``Canvas`` before the assignment deadline. \n",
    "- **Other questions**: If you have doubts/questions about the assignments, feel free to ask them in [this discussion thread](https://canvas.vu.nl/courses/60060/discussion_topics/528048) so that everyone can will be able to see them and our answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a7516d-cce8-46d1-bf16-dcaa399c9f1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8820925d7aa1eb818b8061a7df5d051f",
     "grade": false,
     "grade_id": "initialize",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell before anything else to import the packages you are allowed to used for this assignment\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import sys\n",
    "from io import StringIO\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from numpy.testing import assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ceb3b-da25-40ba-bdef-0e873163d0db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4136fdda7ba31959ae665376240e4755",
     "grade": false,
     "grade_id": "introduction",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 1: Statistical data analysis (15 points)\n",
    "\n",
    "The goal of this part of the assignment is to provide you with practice and experience in some basic data exploration and hypothesis testing with Python. You will work with data from the so-called \"HUE bedtime procrastination study\". A cleaned version of the data is available on Canvas (`hue_week_3.csv`), as well as another file that contains data from the post-study questionnaire that participants filled out at the end of the study (`hue_questionnaire.csv`). This file contains the following information:\n",
    "\n",
    "| Column | Description |\n",
    "-----------------------|--------------------------------------------|\n",
    "| `gender`          | 1 = male, 2 = female |\n",
    "| `age`           | Numeric age value | \n",
    "| `chronotype`      |    Single item (7-point scale), do you consider yourself more of a <br> morning (1) or an evening person? (7) |\n",
    "| `bp_scale` | Dutch version of the Bedtime Procrastination Scale |\n",
    "| `motivation` | Questions pertaining to personality traits related to procrastination. <br> Single item (7-point scale), how motivated were you to go to bed on <br> time each night? (1 = not motivated, 7 = very motivated) |\n",
    "| `daytime_sleepiness` | Dutch translation of the Epworth Sleepiness Scale <br> (4-point scale from 0-3; 8 questions, values summed) |\n",
    "| `self_reported_effectiveness` | Single item (7-point scale), <br> do you feel more rested since the intervention |\n",
    "\n",
    "In this part of the assignment, you will use Python to examine the post-questionnaire data in relation to the HUE data file, visualize trends and relationships, look for correlations between factors, test for significant differences between groups and build a regression model to predict bedtime delay. In order to perform the analyses, a number of transformations on the data still need to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e54771-4b5d-4c57-8808-3caf478e6dd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44c14d72089d931c5a449ebab42380a1",
     "grade": false,
     "grade_id": "Q1-def",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Importing and cleaning the file (0 points)\n",
    "\n",
    "The read-only cell below already implements the ``read_data`` function, which returns a clean dataframe. The precise steps that the function takes are detailed as follows: \n",
    "\n",
    "- Reads the HUE data file and the questionnaire data file into two separate pandas DataFrames.\n",
    "\n",
    "- Creates a new DataFrame that contains the following Series:\n",
    "    \n",
    "| Column | Description |\n",
    "-----------------------|--------------------------------------------|\n",
    "| `ID` | Participant ID |\n",
    "| `group` | Participant group (1 for experimental, 0 for control) |\n",
    "| `delay_nights` | The number of nights the participant delayed their bedtime (range: 0-12) |\n",
    "| `delay_time` | The mean time in seconds a participant delayed their bedtime <br> (total delay in seconds, divided by the number of observations <br> measured for each individual, rounded to nearest second) |\n",
    "| `sleep_time` | The mean in-bed time in seconds of a participant (rounded to nearest second) <br> Average should be taken over the ``In Bed`` columns. Disregard inconsistencies with the difference ``Rise Time`` - ``Bed Time``  |\n",
    "    \n",
    "    \n",
    "- Sets the index of this new DataFrame to `ID`. Note that there should only be a single row per participant ID.    \n",
    "\n",
    "- Fills this new DataFrame by transforming data from the DataFrame about participants' bedtimes (from the HUE data file).\n",
    "\n",
    "- Merges this new DataFrame with the post-questionnaire data and store the resulting DataFrame in a new variable. Perform this merging operation of the two DataFrames in such a way that the resulting Data Frame only contains IDs that were present in both datasets.\n",
    "\n",
    "- Removes the rows that have NaN values in this merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1eb563-bc0a-4fe9-a8a7-f290110c8ac2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba4748769c163f390486a943be98569c",
     "grade": false,
     "grade_id": "Q1-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_data(sleepdatafile, surveydatafile):\n",
    "    \n",
    "    def calculate_number_and_time_delay(row):\n",
    "        numberDelayNights = 0\n",
    "        sumDelayTime = 0\n",
    "        meanDelayTime = 0\n",
    "        isNan = True\n",
    "\n",
    "        i = 4;\n",
    "        while i <= 114:\n",
    "            if not np.isnan(row[i]):\n",
    "                isNan = False\n",
    "                if float(row[i]) >= 0:\n",
    "                    numberDelayNights += 1\n",
    "                    sumDelayTime = sumDelayTime + float(row[i])\n",
    "            i = i + 10\n",
    "        if numberDelayNights > 0:\n",
    "            meanDelayTime = round(sumDelayTime / numberDelayNights, 0)\n",
    "        elif isNan:\n",
    "            meanDelayTime = np.nan\n",
    "            numberDelayNights = np.nan\n",
    "        return numberDelayNights, meanDelayTime\n",
    "\n",
    "\n",
    "    def create_new_dataframe(sleepDf):\n",
    "        df = pd.DataFrame(columns = ['group', 'delay_nights', 'delay_time'])\n",
    "\n",
    "        for index,row in sleepDf.iterrows():\n",
    "            ID = index\n",
    "            group = row[0]\n",
    "            numberDelayNights, meanDelayTime = calculate_number_and_time_delay(row)\n",
    "            df =  df.append(pd.Series({'group':group, 'delay_nights':numberDelayNights, 'delay_time':meanDelayTime}, name = ID))\n",
    "        return df\n",
    "\n",
    "\n",
    "    def merge_dataFrames(transformedDf, surveyDf):\n",
    "        return pd.merge(transformedDf, surveyDf, how = 'inner', left_index = True, right_index = True)\n",
    "\n",
    "\n",
    "    def calculate_sleep_duration(sleepDf, mergedDf):\n",
    "        mergedDf[\"sleep_duration\"] = np.nan\n",
    "\n",
    "        for index,row in sleepDf.iterrows():\n",
    "            numberNights = 0\n",
    "            sumTime = 0\n",
    "            meanTime = 0\n",
    "            isNan = True\n",
    "\n",
    "            if index in mergedDf.index:\n",
    "                i = 7\n",
    "                while i <= 117:\n",
    "                    if not np.isnan(row[i]):\n",
    "                        isNan = False\n",
    "                        numberNights += 1\n",
    "                        sumTime = sumTime + float(row[i])\n",
    "                    i = i + 10\n",
    "\n",
    "                if numberNights > 0:\n",
    "                    meanTime = round(sumTime / numberNights, 0)\n",
    "                elif isNan:\n",
    "                    meanTime = np.nan\n",
    "                mergedDf.at[index, 'sleep_duration']=meanTime\n",
    "\n",
    "    sleepDf = pd.read_csv(sleepdatafile, delimiter = ',', index_col = 0)\n",
    "    surveyDf = pd.read_csv(surveydatafile, delimiter = ',', index_col = 0)\n",
    "    \n",
    "    transformedDf = create_new_dataframe(sleepDf)\n",
    "    mergedDf = merge_dataFrames(transformedDf, surveyDf)\n",
    "    calculate_sleep_duration(sleepDf, mergedDf)\n",
    "    mergedDfNoNan = mergedDf.dropna()\n",
    "    mergedDfNoNan['group'] = mergedDfNoNan['group'].astype('int64')\n",
    "    \n",
    "    return mergedDfNoNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34887b02-eec8-4339-a7e8-2deaafadaf42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5054dc9363235a9475f01b950f7b53fb",
     "grade": false,
     "grade_id": "Q1-check",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>delay_nights</th>\n",
       "      <th>delay_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>chronotype</th>\n",
       "      <th>bp_scale</th>\n",
       "      <th>motivation</th>\n",
       "      <th>daytime_sleepiness</th>\n",
       "      <th>self_reported_effectiveness</th>\n",
       "      <th>sleep_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6030.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6.11</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>31193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2494.0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>5.22</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>27852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4149.0</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6.67</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>30343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>32573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5417.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>5.11</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>29822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>5.56</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>29001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3672.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>26984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3622.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>28370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5707.0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>5.89</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>30725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>31125.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    group  delay_nights  delay_time  gender  age  chronotype  bp_scale  \\\n",
       "1       0          10.0      6030.0       2   20           7      6.11   \n",
       "2       0           7.0      2494.0       2   25           5      5.22   \n",
       "4       1           7.0      4149.0       2   30           1      6.67   \n",
       "5       1           9.0      2620.0       2   27           5      2.67   \n",
       "6       0           7.0      5417.0       1   27           6      5.11   \n",
       "..    ...           ...         ...     ...  ...         ...       ...   \n",
       "52      1           9.0      1947.0       1   18           7      5.56   \n",
       "55      0           5.0      3672.0       1   26           4      5.00   \n",
       "58      0          11.0      3622.0       1   35           7      6.33   \n",
       "61      0           9.0      5707.0       2   39           7      5.89   \n",
       "63      0           6.0      1090.0       2   59           6      4.00   \n",
       "\n",
       "    motivation  daytime_sleepiness  self_reported_effectiveness  \\\n",
       "1            4                  17                            4   \n",
       "2            4                  21                            2   \n",
       "4            5                  14                            5   \n",
       "5            6                  12                            6   \n",
       "6            6                  14                            3   \n",
       "..         ...                 ...                          ...   \n",
       "52           3                  11                            3   \n",
       "55           2                  15                            1   \n",
       "58           6                  12                            2   \n",
       "61           5                  19                            1   \n",
       "63           5                  15                            3   \n",
       "\n",
       "    sleep_duration  \n",
       "1          31193.0  \n",
       "2          27852.0  \n",
       "4          30343.0  \n",
       "5          32573.0  \n",
       "6          29822.0  \n",
       "..             ...  \n",
       "52         29001.0  \n",
       "55         26984.0  \n",
       "58         28370.0  \n",
       "61         30725.0  \n",
       "63         31125.0  \n",
       "\n",
       "[38 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to correctly import the dataframe (and ignore the warnings)\n",
    "sleepdatafile   = 'data/hue/hue_week_3.csv'\n",
    "surveydatafile  = 'data/hue/hue_questionnaire.csv'\n",
    "mergedDf = read_data(sleepdatafile, surveydatafile)\n",
    "display(mergedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ee1c4-4c0c-41cc-bc0c-56483c061505",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66b68a8c290da1acca4e043d8e6451fb",
     "grade": false,
     "grade_id": "Q2-def",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Correlation coefficients (5 points)\n",
    "Use the `scipy.stats` package and, respectively, the Pearson correlation test and the Kendall rank correlation test, to calculate the following correlation coefficients for the dataframe ``mergedDf`` defined above:\n",
    "\n",
    "- the Pearson correlation coefficient between bedtime procrastination scale ( `bp_scale`, a personality trait) and mean time spent delaying bedtime,    \n",
    "\n",
    "- the Pearson correlation coefficient between mean time spent delaying bedtime and daytime sleepiness,\n",
    "\n",
    "- the Kendall rank correlation coefficient between age and mean time spent delaying bedtime.\n",
    "\n",
    "Save them into the variables `r1`, `r2`, `tau` without any rounding. Save also the respective p-values into the variables `pvalue1`, `pvalue2`, `pvalue3` (also without any rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfcac9e-264a-4d4e-8936-aabe0970c064",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a86c366dbae18a412af00c812ca06c60",
     "grade": false,
     "grade_id": "Q2-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_correlations(mergedDf):\n",
    "    r1, pvalue1 = stats.pearsonr(mergedDf['bp_scale'], mergedDf['delay_time'])\n",
    "    r2, pvalue2 = stats.pearsonr(mergedDf['delay_time'], mergedDf['daytime_sleepiness'])\n",
    "    tau, pvalue3 = stats.kendalltau(mergedDf['age'], mergedDf['delay_time'])\n",
    "    \n",
    "    return  r1, pvalue1, r2, pvalue2, tau, pvalue3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d0914c-957b-44e8-ae72-83be63ba7b54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e603cc161b709bee91da87ff04a0a661",
     "grade": true,
     "grade_id": "Q2-check",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation tests:\n",
      "\n",
      "The value of the test statistic is: 0.5926593048409182\n",
      "The p-value is: 8.837285991335382e-05 \n",
      "\n",
      "The value of the test statistic is: 0.08212367473495859\n",
      "The p-value is: 0.6240188835001698 \n",
      "\n",
      "The value of the test statistic is: -0.08238035084305242\n",
      "The p-value is: 0.4725232598663073 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r1, pvalue1, r2, pvalue2, tau, pvalue3 = calculate_correlations(mergedDf)\n",
    "\n",
    "statistics = [r1,r2,tau]\n",
    "pvalues = [pvalue1, pvalue2, pvalue3]\n",
    "\n",
    "print(\"Correlation tests:\\n\")\n",
    "for (statistic, pvalue) in zip(statistics, pvalues):\n",
    "    print('The value of the test statistic is:',statistic)\n",
    "    print('The p-value is:', pvalue,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3b3d3-1a24-4f66-878d-8cdeae2bd43e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbde71eddaa1f773f516aafd483b8141",
     "grade": false,
     "grade_id": "Q3-def",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Significant differences (5 points)\n",
    "\n",
    "We keep analyzing the dataframe ``mergedDf`` defined above. Use the `scipy.stats` package to determine whether there are significant differences (at 5\\% significance level) between the experimental group and the control group in terms of:\n",
    "<br></li>\n",
    "<li>\n",
    "    the time participants spent in bed each night,   \n",
    "<br></li>\n",
    "<li>\n",
    "    the number of nights participants delayed their bedtime,\n",
    "<br></li>\n",
    "<li>\n",
    "    the mean time participants spent delaying their bedtime.\n",
    "</li>\n",
    "</ul> \n",
    "\n",
    "Use the t-test or the Wilcoxon rank-sum test to reach a conclusion and use knowledge gained in the courses Statistics and Statistical Data Analysis to determine which statistical test is appropriate. Save the conclusions - either the string `'significant difference'` or `'no significant difference'` - into the variables `diff1`, `diff2`, `diff3`.\n",
    "\n",
    "\\* Note that in this exercise you are not expected to explicitly motivate the choice of an appropriate test, but you will be in the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7215c01f-beae-4039-90e4-67f8ce6d3529",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49b8eae18572cfba57898e94b12c8a9f",
     "grade": false,
     "grade_id": "Q3-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perform_tests(mergedDf):\n",
    "    test1 = stats.ranksums(mergedDf[mergedDf['group'] == 1]['sleep_duration'],mergedDf[mergedDf['group'] == 0]['sleep_duration'])\n",
    "    test2 = stats.ranksums(mergedDf[mergedDf['group'] == 1]['delay_nights'],mergedDf[mergedDf['group'] == 0]['delay_nights'])\n",
    "    test3 = stats.ranksums(mergedDf[mergedDf['group'] == 1]['delay_time'],mergedDf[mergedDf['group'] == 0]['delay_time'])\n",
    "    \n",
    "    def reject(test):\n",
    "        if test.pvalue < 0.05:\n",
    "            return 'significant difference'\n",
    "        else:\n",
    "            return 'no significant difference'\n",
    "        \n",
    "    diff1 = reject(test1)\n",
    "    diff2 = reject(test2)\n",
    "    diff3 = reject(test3)\n",
    "    \n",
    "    return diff1,diff2,diff3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd35a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.9111424088478088, pvalue=0.09011896699666977)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm.qqplot(mergedDf[mergedDf['group'] == 1]['delay_time'], line='45',fit= True)\n",
    "stats.shapiro(mergedDf[mergedDf['group'] == 1]['delay_time'])\n",
    "# plt.hist(mergedDf[mergedDf['group'] == 1]['delay_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60143b1b-98da-4bcf-8883-8567021fbc67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "575a9f10486748ade2f6d589bb6ede35",
     "grade": true,
     "grade_id": "Q3-check",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time participants spent in bed each night: no significant difference\n",
      "The number of nights participants delayed their bedtime: no significant difference\n",
      "The mean time participants spent delaying their bedtime: significant difference\n"
     ]
    }
   ],
   "source": [
    "diff1, diff2, diff3 = perform_tests(mergedDf) \n",
    "\n",
    "print('The time participants spent in bed each night:', diff1)\n",
    "print('The number of nights participants delayed their bedtime:', diff2)\n",
    "print('The mean time participants spent delaying their bedtime:', diff3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e75ab0-f02c-44e6-9cec-0c08e1f1fba5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbade902188b65b1e02429cfd4aca28c",
     "grade": false,
     "grade_id": "Q4-def",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ordinary Least Squares regression (5 points)\n",
    "\n",
    "Also for this last question, you will use the dataframe ``mergedDf`` defined above. Use `statsmodels.api` to build a (OLS) regression model for `delay_time` on the predictors `age`, `bp_scale`, and  `chronotype` (in this order!). \n",
    "\n",
    "Return the coefficients and the conclusion on significance of the model. More specifically, into the variable `parameters` directly save the output of `*.params` applied to the model (without any rounding), and into the variable `conclusion` save the string 'significant' or 'not significant'.\n",
    "\n",
    "\\* Convince yourself that the basic diagnostics for this model are OK. Note that here you are not expected to explicitly check the diagnostics, but you will be in the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c72082-2690-4cdf-9c68-96c4525f8f19",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dd7bd5c351e7fe1c8371561b56087af",
     "grade": false,
     "grade_id": "Q4-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def regression_analysis(mergedDf):\n",
    "    X = np.column_stack((mergedDf['age'],mergedDf['bp_scale'],mergedDf['chronotype']))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(mergedDf['delay_time'],X)\n",
    "    results = model.fit()\n",
    "    parameters = results.params\n",
    "    \n",
    "    # F test pvalue\n",
    "    if results.f_pvalue < 0.05:\n",
    "        conclusion = 'significant' \n",
    "    else:\n",
    "        conclusion = 'not significant'\n",
    "        \n",
    "    return parameters,conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72eb642-faf7-46ef-9860-d3b27634855c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f6ee5bc22c1acbba6e60f4e33ed70fa",
     "grade": true,
     "grade_id": "Q4-check",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters of the model are:\n",
      "const   -1399.595480\n",
      "x1        -25.178161\n",
      "x2        882.064704\n",
      "x3         99.552661\n",
      "dtype: float64\n",
      "\n",
      "The model is significant\n"
     ]
    }
   ],
   "source": [
    "parameters, conclusion = regression_analysis(mergedDf)\n",
    "\n",
    "print('The parameters of the model are:')\n",
    "print(parameters)\n",
    "print('\\nThe model is', conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a502fe-7041-4b46-9090-ac5577ab1727",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4aa675d849a39edf5a2985cbb4c6b1fa",
     "grade": false,
     "grade_id": "E2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 2: Feature engineering (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666810a6-5e5d-460f-a0f1-9faa1c61324c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94242434e2240928fa8714aa4937f788",
     "grade": false,
     "grade_id": "E2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Date one-hot encoding (5 points)\n",
    "The dataframe below contains time-series data related to weather measurements from January - October 2020. In this exercise, you will extract date related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be41596-a7a1-42ef-af13-794236b8d41b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2bb49ec0e498b242899046da8f39d7b",
     "grade": false,
     "grade_id": "E2a-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Station</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>863598</td>\n",
       "      <td>RSM00027719</td>\n",
       "      <td>170.0</td>\n",
       "      <td>54.2330</td>\n",
       "      <td>37.6170</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>261834</td>\n",
       "      <td>CA004014156</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>51.4167</td>\n",
       "      <td>-105.2500</td>\n",
       "      <td>497.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>74821</td>\n",
       "      <td>RSM00028224</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>58.0167</td>\n",
       "      <td>56.3000</td>\n",
       "      <td>171.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>470892</td>\n",
       "      <td>CA00703GDKB</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>46.0833</td>\n",
       "      <td>-74.5500</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>149342</td>\n",
       "      <td>SPW00014010</td>\n",
       "      <td>127.0</td>\n",
       "      <td>41.6667</td>\n",
       "      <td>-1.0333</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Unnamed: 0      Station   TAVG  Latitude  Longitude  Elevation  \\\n",
       "0  2020-08-26      863598  RSM00027719  170.0   54.2330    37.6170      204.0   \n",
       "1  2020-03-12      261834  CA004014156  -71.0   51.4167  -105.2500      497.0   \n",
       "2  2020-01-21       74821  RSM00028224  -52.0   58.0167    56.3000      171.0   \n",
       "3  2020-05-09      470892  CA00703GDKB  -12.0   46.0833   -74.5500      239.0   \n",
       "4  2020-02-10      149342  SPW00014010  127.0   41.6667    -1.0333      263.0   \n",
       "\n",
       "   PRCP  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   8.0  \n",
       "3   0.0  \n",
       "4   0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/weather2020.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd7eca6-eec0-49b2-9c04-be5ba22af729",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27640205184731e374277f93b5df1e55",
     "grade": false,
     "grade_id": "E2a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_date(df):\n",
    "    \"\"\"\n",
    "    Infer the following information from the dataframe:\n",
    "    - The weekday (e.g., Monday, Tuesday, etc.)\n",
    "    - The week number (e.g., 'Week 1', 'Week 2', etc.)\n",
    "    - The month (e.g., January, February, etc.)\n",
    "    \n",
    "    Return a new dataframe, which includes all original columns and all new one-hot encoded columns.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Do not modify the original dataframe.\n",
    "    - You only have to add columns for the weeks and months that are included in the dataset.\n",
    "        e.g., \"November\" is not part of the data and thus doesn't need to be included.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    new_df['Date'] = pd.to_datetime(new_df['Date'])\n",
    "    new_df['Weekday'] = new_df['Date'].dt.day_name().astype(str)\n",
    "    new_df['Week number'] = 'Week ' + new_df['Date'].dt.isocalendar().week.astype(str)\n",
    "    new_df['Month'] = new_df['Date'].dt.month_name().astype(str)\n",
    "    \n",
    "    # Get one hot encoding of columns B\n",
    "    weekdays = pd.get_dummies(new_df['Weekday'])\n",
    "    weeknum = pd.get_dummies(new_df['Week number'])\n",
    "    month = pd.get_dummies(new_df['Month'])\n",
    "    \n",
    "    # Drop column B as it is now encoded\n",
    "    new_df = new_df.drop(['Weekday', 'Week number', 'Month'],axis = 1)\n",
    "    # Join the encoded df\n",
    "    new_df = new_df.join(weekdays, how='inner').join(weeknum, how='inner').join(month, how='inner')\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0141c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 831 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Station</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>...</th>\n",
       "      <th>April</th>\n",
       "      <th>August</th>\n",
       "      <th>February</th>\n",
       "      <th>January</th>\n",
       "      <th>July</th>\n",
       "      <th>June</th>\n",
       "      <th>March</th>\n",
       "      <th>May</th>\n",
       "      <th>October</th>\n",
       "      <th>September</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>863598</td>\n",
       "      <td>RSM00027719</td>\n",
       "      <td>170.0</td>\n",
       "      <td>54.2330</td>\n",
       "      <td>37.6170</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>261834</td>\n",
       "      <td>CA004014156</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>51.4167</td>\n",
       "      <td>-105.2500</td>\n",
       "      <td>497.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>74821</td>\n",
       "      <td>RSM00028224</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>58.0167</td>\n",
       "      <td>56.3000</td>\n",
       "      <td>171.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>470892</td>\n",
       "      <td>CA00703GDKB</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>46.0833</td>\n",
       "      <td>-74.5500</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>149342</td>\n",
       "      <td>SPW00014010</td>\n",
       "      <td>127.0</td>\n",
       "      <td>41.6667</td>\n",
       "      <td>-1.0333</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212852</th>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>359708</td>\n",
       "      <td>FIE00146783</td>\n",
       "      <td>37.0</td>\n",
       "      <td>68.8489</td>\n",
       "      <td>28.3039</td>\n",
       "      <td>123.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212853</th>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>241960</td>\n",
       "      <td>SWM00002468</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>56.8500</td>\n",
       "      <td>14.8300</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212854</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>481163</td>\n",
       "      <td>CA003014690</td>\n",
       "      <td>58.0</td>\n",
       "      <td>53.7167</td>\n",
       "      <td>-111.1167</td>\n",
       "      <td>580.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212855</th>\n",
       "      <td>2020-05-25</td>\n",
       "      <td>529589</td>\n",
       "      <td>USS0011G30S</td>\n",
       "      <td>73.0</td>\n",
       "      <td>42.5200</td>\n",
       "      <td>-111.9600</td>\n",
       "      <td>2392.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212856</th>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>170527</td>\n",
       "      <td>GME00102268</td>\n",
       "      <td>139.0</td>\n",
       "      <td>51.2969</td>\n",
       "      <td>6.7700</td>\n",
       "      <td>37.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212857 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Unnamed: 0      Station   TAVG  Latitude  Longitude  \\\n",
       "0      2020-08-26      863598  RSM00027719  170.0   54.2330    37.6170   \n",
       "1      2020-03-12      261834  CA004014156  -71.0   51.4167  -105.2500   \n",
       "2      2020-01-21       74821  RSM00028224  -52.0   58.0167    56.3000   \n",
       "3      2020-05-09      470892  CA00703GDKB  -12.0   46.0833   -74.5500   \n",
       "4      2020-02-10      149342  SPW00014010  127.0   41.6667    -1.0333   \n",
       "...           ...         ...          ...    ...       ...        ...   \n",
       "212852 2020-04-08      359708  FIE00146783   37.0   68.8489    28.3039   \n",
       "212853 2020-03-06      241960  SWM00002468  -32.0   56.8500    14.8300   \n",
       "212854 2020-05-12      481163  CA003014690   58.0   53.7167  -111.1167   \n",
       "212855 2020-05-25      529589  USS0011G30S   73.0   42.5200  -111.9600   \n",
       "212856 2020-02-16      170527  GME00102268  139.0   51.2969     6.7700   \n",
       "\n",
       "        Elevation  PRCP  Friday  Monday  ...  April  August  February  \\\n",
       "0           204.0   0.0       0       0  ...      0       1         0   \n",
       "1           497.0   0.0       0       0  ...      0       0         0   \n",
       "2           171.0   8.0       0       0  ...      0       0         0   \n",
       "3           239.0   0.0       0       0  ...      0       0         0   \n",
       "4           263.0   0.0       0       1  ...      0       0         1   \n",
       "...           ...   ...     ...     ...  ...    ...     ...       ...   \n",
       "212852      123.0  20.0       0       0  ...      1       0         0   \n",
       "212853      199.0   0.0       1       0  ...      0       0         0   \n",
       "212854      580.0   0.0       0       0  ...      0       0         0   \n",
       "212855     2392.7   0.0       0       1  ...      0       0         0   \n",
       "212856       37.0  92.0       0       0  ...      0       0         1   \n",
       "\n",
       "        January  July  June  March  May  October  September  \n",
       "0             0     0     0      0    0        0          0  \n",
       "1             0     0     0      1    0        0          0  \n",
       "2             1     0     0      0    0        0          0  \n",
       "3             0     0     0      0    1        0          0  \n",
       "4             0     0     0      0    0        0          0  \n",
       "...         ...   ...   ...    ...  ...      ...        ...  \n",
       "212852        0     0     0      0    0        0          0  \n",
       "212853        0     0     0      1    0        0          0  \n",
       "212854        0     0     0      0    1        0          0  \n",
       "212855        0     0     0      0    1        0          0  \n",
       "212856        0     0     0      0    0        0          0  \n",
       "\n",
       "[212857 rows x 68 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time one_hot_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf685b7b-ae09-45d9-8a33-e70021b3dd47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4882a65e0fa7c54aeee57ff050724559",
     "grade": true,
     "grade_id": "E2a-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "oh = one_hot_date(df)\n",
    "assert_equal(oh.loc[161231, ['Wednesday', 'Week 1', 'January']].values, [1, 1, 1])\n",
    "assert_equal(oh.loc[161231, ['Monday', 'Week 2', 'February']].values, [0, 0, 0])\n",
    "assert_equal(oh.loc[0, ['Wednesday', 'Week 35', 'August']].values, [1, 1, 1])\n",
    "assert_equal(oh.loc[0, ['Monday', 'Week 36', 'September']].values, [0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaeb644-d64c-4408-ad54-0ed1f7291403",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ff815d771057437b2bb0aa6fc5c8d95",
     "grade": false,
     "grade_id": "E2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Categorical one-hot encoding (5 points)\n",
    "\n",
    "Below is a more recent version of movies table from the movielens data set. In contrast to the version from the previous assignment, the genres are not one-hot encoded in this more recent version. The goal is to one-hot encode these genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f885bb48-bae0-4076-9ae9-3c6eec7a8603",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1536e7713a2ac7d24bafd75fae9f7a3e",
     "grade": false,
     "grade_id": "E2b-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "movieId                                       \n",
       "1                          Toy Story (1995)   \n",
       "2                            Jumanji (1995)   \n",
       "3                   Grumpier Old Men (1995)   \n",
       "4                  Waiting to Exhale (1995)   \n",
       "5        Father of the Bride Part II (1995)   \n",
       "\n",
       "                                              genres  \n",
       "movieId                                               \n",
       "1        Adventure|Animation|Children|Comedy|Fantasy  \n",
       "2                         Adventure|Children|Fantasy  \n",
       "3                                     Comedy|Romance  \n",
       "4                               Comedy|Drama|Romance  \n",
       "5                                             Comedy  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('data/movielens/movies.csv', index_col='movieId')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1a872-8f10-4023-9f4c-d858d4727857",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cba6d3596fc2e178cbb82b3d6d5176c0",
     "grade": false,
     "grade_id": "E2b-extra",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "```\n",
    "Movies Data File Structure (movies.csv)\n",
    "---------------------------------------\n",
    "\n",
    "Movie information is contained in the file `movies.csv`. Each line of this file after the header row represents one movie, and has the following format:\n",
    "\n",
    "    movieId,title,genres\n",
    "\n",
    "Movie titles are entered manually or imported from <https://www.themoviedb.org/>, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n",
    "\n",
    "Genres are a pipe-separated list.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5037fa1c-7634-4f53-bcd3-3509f0a04ec6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5f4b3f5173bd854440c646e809760ad",
     "grade": false,
     "grade_id": "E2b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ohe_genres(movies):\n",
    "    \"\"\"\n",
    "    Return a new dataframe, which includes all original columns and one-hot encoded genre columns.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Do not modify the original dataframe.\n",
    "    \"\"\"\n",
    "    movies_copy = movies.copy()\n",
    "    movies_copy = movies_copy.join(movies_copy['genres'].str.get_dummies('|'), how='inner')\n",
    "    \n",
    "    return movies_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "625a6660-06d4-4e7f-8a3d-e4a32f8dffb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84e87ac9a8a443a204fa807e9d5a2555",
     "grade": true,
     "grade_id": "E2b-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(ohe_genres(movies).loc[1, ['Adventure', 'Animation', 'Children']].values, [1, 1, 1])\n",
    "assert_equal(ohe_genres(movies).loc[2, ['Adventure', 'Animation', 'Children']].values, [1, 0, 1])\n",
    "assert_equal(ohe_genres(movies).loc[193581, ['Thriller', 'War', 'Western']].values, [0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da348ae-1765-48dc-840c-3c836bb23470",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac4d6da41dd838cc81a63a23e5edb1de",
     "grade": false,
     "grade_id": "E2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Custom features\n",
    "For this question, we use the new version of the movielens ratings table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f23048-cba5-4a2f-9e0a-4707e79ea2c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd77f2e680f4ef1687b986352d87b116",
     "grade": false,
     "grade_id": "E2c-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('data/movielens/ratings.csv')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec1c8d2e-9828-4187-9bc8-3275f79b6198",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a346a52d8f2031332338106a29c190",
     "grade": false,
     "grade_id": "E2c-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def custom_features(df, funcs):\n",
    "    \"\"\"\n",
    "    Return a dataframe with custom features using the passed-in funcs. \n",
    "    \n",
    "    The name of each new column feature should be the function call with column name as argument.\n",
    "    For example, 'log(userId)'. Hint: You can use f.__name__ to get the name of a function.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - The original dataframe should also be included. \n",
    "    - Do not modify the original dataframe.\n",
    "    \"\"\"\n",
    "    ratings_copy = df.copy()\n",
    "\n",
    "    for i in funcs:\n",
    "        ndf = ratings.apply(i)\n",
    "        ndf.columns = [i.__name__ + '(' + x +')' for x in ndf.columns]\n",
    "        ratings_copy = ratings_copy.join(ndf, how = 'inner')\n",
    "    \n",
    "    return ratings_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2c30c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>log(userId)</th>\n",
       "      <th>log(movieId)</th>\n",
       "      <th>log(rating)</th>\n",
       "      <th>log(timestamp)</th>\n",
       "      <th>cos(userId)</th>\n",
       "      <th>cos(movieId)</th>\n",
       "      <th>cos(rating)</th>\n",
       "      <th>cos(timestamp)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>20.687621</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>-0.653644</td>\n",
       "      <td>0.422204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>20.687619</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>-0.989992</td>\n",
       "      <td>-0.653644</td>\n",
       "      <td>-0.953039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>20.687620</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.960170</td>\n",
       "      <td>-0.653644</td>\n",
       "      <td>0.941752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.850148</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>20.687622</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>-0.992335</td>\n",
       "      <td>0.283662</td>\n",
       "      <td>0.530910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>20.687621</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.964966</td>\n",
       "      <td>0.283662</td>\n",
       "      <td>-0.979799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "      <td>6.413459</td>\n",
       "      <td>12.022955</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>21.124621</td>\n",
       "      <td>0.862288</td>\n",
       "      <td>-0.252991</td>\n",
       "      <td>-0.653644</td>\n",
       "      <td>-0.838313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "      <td>6.413459</td>\n",
       "      <td>12.033194</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>21.124623</td>\n",
       "      <td>0.862288</td>\n",
       "      <td>-0.999985</td>\n",
       "      <td>0.283662</td>\n",
       "      <td>0.181653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "      <td>6.413459</td>\n",
       "      <td>12.033206</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>21.124906</td>\n",
       "      <td>0.862288</td>\n",
       "      <td>0.421085</td>\n",
       "      <td>0.283662</td>\n",
       "      <td>0.056757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "      <td>6.413459</td>\n",
       "      <td>12.033218</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>21.124620</td>\n",
       "      <td>0.862288</td>\n",
       "      <td>0.649519</td>\n",
       "      <td>0.283662</td>\n",
       "      <td>0.634537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "      <td>6.413459</td>\n",
       "      <td>12.048688</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>21.124620</td>\n",
       "      <td>0.862288</td>\n",
       "      <td>-0.805677</td>\n",
       "      <td>-0.989992</td>\n",
       "      <td>0.496240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp  log(userId)  log(movieId)  \\\n",
       "0            1        1     4.0   964982703     0.000000      0.000000   \n",
       "1            1        3     4.0   964981247     0.000000      1.098612   \n",
       "2            1        6     4.0   964982224     0.000000      1.791759   \n",
       "3            1       47     5.0   964983815     0.000000      3.850148   \n",
       "4            1       50     5.0   964982931     0.000000      3.912023   \n",
       "...        ...      ...     ...         ...          ...           ...   \n",
       "100831     610   166534     4.0  1493848402     6.413459     12.022955   \n",
       "100832     610   168248     5.0  1493850091     6.413459     12.033194   \n",
       "100833     610   168250     5.0  1494273047     6.413459     12.033206   \n",
       "100834     610   168252     5.0  1493846352     6.413459     12.033218   \n",
       "100835     610   170875     3.0  1493846415     6.413459     12.048688   \n",
       "\n",
       "        log(rating)  log(timestamp)  cos(userId)  cos(movieId)  cos(rating)  \\\n",
       "0          1.386294       20.687621     0.540302      0.540302    -0.653644   \n",
       "1          1.386294       20.687619     0.540302     -0.989992    -0.653644   \n",
       "2          1.386294       20.687620     0.540302      0.960170    -0.653644   \n",
       "3          1.609438       20.687622     0.540302     -0.992335     0.283662   \n",
       "4          1.609438       20.687621     0.540302      0.964966     0.283662   \n",
       "...             ...             ...          ...           ...          ...   \n",
       "100831     1.386294       21.124621     0.862288     -0.252991    -0.653644   \n",
       "100832     1.609438       21.124623     0.862288     -0.999985     0.283662   \n",
       "100833     1.609438       21.124906     0.862288      0.421085     0.283662   \n",
       "100834     1.609438       21.124620     0.862288      0.649519     0.283662   \n",
       "100835     1.098612       21.124620     0.862288     -0.805677    -0.989992   \n",
       "\n",
       "        cos(timestamp)  \n",
       "0             0.422204  \n",
       "1            -0.953039  \n",
       "2             0.941752  \n",
       "3             0.530910  \n",
       "4            -0.979799  \n",
       "...                ...  \n",
       "100831       -0.838313  \n",
       "100832        0.181653  \n",
       "100833        0.056757  \n",
       "100834        0.634537  \n",
       "100835        0.496240  \n",
       "\n",
       "[100836 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_features(ratings, [np.log, np.cos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbacaa03-9b9b-4243-ab34-df856998e4c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b55353ea51bff1ef9e70fc515b9960c",
     "grade": true,
     "grade_id": "E2c-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "result = custom_features(ratings, [np.log, np.cos])\n",
    "\n",
    "assert('userId' in result.columns)\n",
    "assert('log(userId)' in result.columns)\n",
    "assert('cos(userId)' in result.columns)\n",
    "assert_almost_equal(result.loc[0, ['userId', 'log(userId)', 'cos(userId)']], [1, 0, 0.540302], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d09d7-5792-4b53-82e7-63c8703740af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d372879f6862c0777024403519b1ea2",
     "grade": false,
     "grade_id": "E3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 3: Classification SPAM vs. HAM (35 points)\n",
    "\n",
    "The goal of this exercise is to build a classifier that is able to recognize spam email. \"Ham\" is e-mail that is not spam. The email data is provided in the following dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4267570d-1c91-473e-ae70-9f47719092e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "508e7022f23e30838284427c363eb0fc",
     "grade": false,
     "grade_id": "E3-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data/spamham/lokay-m/tw_commercial_group/1701</th>\n",
       "      <td>Happy New Year Gang!\\n\\n\\n\\nHere is a draft fo...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/spamham/BG/2005/03/1111094580.25911_7.txt</th>\n",
       "      <td>----PJDPMV.YNLGEUXDFUJ\\n\\nContent-Type: text/p...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/spamham/BG/2004/08/1093039901.5282_162.txt</th>\n",
       "      <td>&lt;html&gt;\\n\\n&lt;body&gt;\\n\\n&lt;center&gt;\\n\\n&lt;font face=\"ve...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/spamham/GP/part10/msg1932.eml</th>\n",
       "      <td>&lt;html&gt;\\n\\n&lt;body&gt;\\n\\n&lt;div align=3D\"center\"&gt;&lt;br&gt;...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/spamham/kaminski-v/personal/101</th>\n",
       "      <td>Samer,\\n\\n\\n\\nI am glad this problem has been ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "data/spamham/lokay-m/tw_commercial_group/1701    Happy New Year Gang!\\n\\n\\n\\nHere is a draft fo...   \n",
       "data/spamham/BG/2005/03/1111094580.25911_7.txt   ----PJDPMV.YNLGEUXDFUJ\\n\\nContent-Type: text/p...   \n",
       "data/spamham/BG/2004/08/1093039901.5282_162.txt  <html>\\n\\n<body>\\n\\n<center>\\n\\n<font face=\"ve...   \n",
       "data/spamham/GP/part10/msg1932.eml               <html>\\n\\n<body>\\n\\n<div align=3D\"center\"><br>...   \n",
       "data/spamham/kaminski-v/personal/101             Samer,\\n\\n\\n\\nI am glad this problem has been ...   \n",
       "\n",
       "                                                class  \n",
       "data/spamham/lokay-m/tw_commercial_group/1701     ham  \n",
       "data/spamham/BG/2005/03/1111094580.25911_7.txt   spam  \n",
       "data/spamham/BG/2004/08/1093039901.5282_162.txt  spam  \n",
       "data/spamham/GP/part10/msg1932.eml               spam  \n",
       "data/spamham/kaminski-v/personal/101              ham  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/spamham.csv', sep='{', index_col=0).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11ec7d-315c-4fba-8329-ec10a53cb72c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ecb0123c1bcc3964d1938fda1ff370c",
     "grade": false,
     "grade_id": "E3-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The data is quite simple. The `text` column contains the email content and `class` contains the corresponding labels, i.e., whether the email is `spam` or `ham`. In the following, we will use the variables `X` and `y` to denote the text features matrix and target vector, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "694bd0b9-a122-4caa-81a0-0565cb8a1866",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6eb9e26f95e7ce2d866f69fa4ec5ddb",
     "grade": false,
     "grade_id": "E3-import2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = df['text'].values\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a959c35-ece1-460a-8be6-ed09cc333e38",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0195ed1c5d9e73ba5a90b7c96b8f33a",
     "grade": false,
     "grade_id": "E3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Warm-up exercises (15 points)\n",
    "The following five exercises are warm-up exercises to get you started with building a simple Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419d19c-3c86-43ad-aa41-e9a70463a668",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c52489eff6a1fb79c961b3d033b53c4",
     "grade": false,
     "grade_id": "E3a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Count vectorizer (3 points)\n",
    "Recall from the notebook `05.05 Naive Bayes Classifier` that we can extract features from text by counting the frequency of words for each piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a2c033a3-5414-4d2f-a7c8-84b5dee556bd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c71ef23e1ecb05133a629ce1d828472",
     "grade": false,
     "grade_id": "E3a1-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_counts(X):\n",
    "    \"\"\"\n",
    "    Use CountVectorizer from sklearn to concert the dataset to a matrix of token counts.\n",
    "    Return the extracted features.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer()\n",
    "    count = vec.fit_transform(X)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fe420471-7964-4f4d-b130-8fb35ddc47d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95cd1a919c7efd21bfb8d2c31a02d8cc",
     "grade": true,
     "grade_id": "E3a1-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "counts = make_counts(X)\n",
    "assert_equal(counts[0, :].toarray().sum(), 34)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13546cbb-921b-4118-a22e-ca537ea216f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8c3d4b4cfc91c7dfcf8d558ce1bb32e",
     "grade": false,
     "grade_id": "E3a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multinomial Naive Bayes (3 points)\n",
    "Now that we have a `counts` features matrix, we can use this to train a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "432cecdd-db73-4abe-a58b-3c1c28a44f4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e59d08b68aaac09daac9544b54ff8de5",
     "grade": false,
     "grade_id": "E3a2-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def nb_spamham(counts, y):\n",
    "    \"\"\"\n",
    "    Return a fitted a Multinomial Naive-Bayes model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    return model.fit(counts,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9981b4c5-ea84-40b2-8f81-fef9300002a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "911b10f962394d0589d80d621ba53700",
     "grade": true,
     "grade_id": "E3a2-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "counts = count_vectorizer.fit_transform(X)\n",
    "model = nb_spamham(counts, y)\n",
    "\n",
    "examples = ['Free Amazon gift card! Click here!', \"I'm going to attend the Linux users group tomorrow.\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predicted = model.predict(example_counts)\n",
    "\n",
    "assert_equal(predicted, ['spam', 'ham'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04098b-b51b-428b-8e75-647c7418fd84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da4d2998d6170aee4329bf2b50cbb0c8",
     "grade": false,
     "grade_id": "E3a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Pipelining (3 points)\n",
    "The previous series of steps can be connected together into one object which you train and then use to make predictions - this is called *pipelining*. Pipelining simplifies things a lot when you start tweaking your model to improve your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6c77d859-2d86-4532-9a29-33128fe35cb3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffd788f20d9e66f1dee67261a87dd51f",
     "grade": false,
     "grade_id": "E3a3-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "def pipeline_spamham(X, y):\n",
    "    \"\"\"\n",
    "    Use `sklearn.pipeline.make_pipeline` and return a pipeline in which:\n",
    "    - CountVectorizer is used to create the features matrix,\n",
    "    - Multinomial Naive Bayes is used as prediction model.\n",
    "    \n",
    "    Make sure to return the fitted pipeline.\n",
    "    \"\"\"\n",
    "    model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "    \n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "957edcf0-dcb2-40b3-ba14-6e0cdf979f71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e69616e5253dd09b7717104ed900b7c",
     "grade": true,
     "grade_id": "E3a3-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline_spamham(X, y)\n",
    "\n",
    "# Note that we do not need to use CountVectorized.transform here on the examples, because the pipeline takes care of that.\n",
    "examples = [\"Free Amazon gift card! Click here!\", \"I'm going to attend the Linux users group tomorrow.\"]\n",
    "predicted = pipeline.predict(examples)\n",
    "\n",
    "assert_equal(predicted, ['spam', 'ham'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921a660-07d6-48e3-9e8c-fb3bfb17e1e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac607e6822f82456e51945787033e6ec",
     "grade": false,
     "grade_id": "E3a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Metrics: F1-score and confusion matrix (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40cff13-8c3e-4c5d-b23e-79fd37bde50e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "846ad124caf8563243166596faf33235",
     "grade": false,
     "grade_id": "E3a4-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we need to get some real performance metrics to evaluate whether or ont our model predicts well. In binary classification, the [*F-score*](https://en.wikipedia.org/wiki/F-score) is a measure of a test's accuracy. The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is closely related, which is a specific table layout that allows visualization of the classification performance. The next function will ask to compute both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a2c96f4b-b5ff-47c7-932f-1cd62ef50921",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da18c5f0ff63fc4bc9bd2de8bed70c63",
     "grade": false,
     "grade_id": "E3a4-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def metrics(y, predicted):\n",
    "    \"\"\"\n",
    "    Return the F1 score and the confusion matrix.\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y, predicted, pos_label='spam')\n",
    "    matrix = confusion_matrix(y, predicted, labels=['ham', 'spam'])\n",
    "    \n",
    "    return f1, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e75bfbfe-ef3a-4fdb-9fca-19cd8f69f852",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "281195fa58b73bb8f3903c7f51d785cf",
     "grade": true,
     "grade_id": "E3a4-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9419986688219073\n",
      "[[7541   57]\n",
      " [1163 9907]]\n"
     ]
    }
   ],
   "source": [
    "predicted = pipeline.predict(X)\n",
    "f1, confusion = metrics(y, predicted)\n",
    "print(f1)\n",
    "print(confusion)\n",
    "\n",
    "assert_almost_equal(f1, 0.942, 2)\n",
    "assert_equal(confusion[0, 0], 7541)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37526573-3dc5-4b2f-9d61-2d22776334e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "684ebe835d6cf3563e7bee7b173b6d2b",
     "grade": false,
     "grade_id": "E3a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Cross-validate (3 points)\n",
    "So far, we have trained on the entire data set and evaluated our model on the same data. This is of course problematic because it surely will lead to overfitting. To solve this issue, we need to separate our data in testing and holdout sets. In this exercise, we will perform k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f6929b6e-fc3a-46f5-a717-4363f4de2878",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c31df7562278f83c7545819441853b51",
     "grade": false,
     "grade_id": "E3a5-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_spamham(X, y, k=2):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation using sklearn.model_selection.KFold,\n",
    "    where k is an input parameter. \n",
    "    \n",
    "    Return the average F1 score and the sum of confusion matrix over all test/holdout sets.\n",
    "    \"\"\"  \n",
    "    kf = KFold(n_splits=k)\n",
    "    conf_matrix_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "#     predict = cross_val_predict(pipeline,X,y,cv=kf)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        pipeline = pipeline_spamham(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        f1, conf_matrix = metrics(y_test, y_pred)\n",
    "        f1_list.append(f1)\n",
    "        conf_matrix_list.append(conf_matrix)\n",
    "\n",
    "    return np.mean(f1_list), np.sum(conf_matrix_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5b3b8c33-44d7-4ad0-b142-cc91039c20ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4554cfbd0024b06e9360397f0c7a728a",
     "grade": true,
     "grade_id": "E3a5-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9186652371009913\n",
      "[[389   4]\n",
      " [ 87 520]]\n"
     ]
    }
   ],
   "source": [
    "f1, confusion = cv_spamham(X[:1000], y[:1000], 2)\n",
    "print(f1)\n",
    "print(confusion)\n",
    "\n",
    "assert_almost_equal(f1, 0.919, 3)\n",
    "assert_equal(confusion[0, 0], 389)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0041c-645e-45a1-85e3-3815bd5b8bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40be739294c9e5e5b23f4b2a4ebd486b",
     "grade": false,
     "grade_id": "E3a-wrapup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This ends the warm-up exercise. A quick summary of what we did:\n",
    "- We implemented a simple Multinomial Naive-Bayes model;\n",
    "- We computed the F1-score and confusion matrices to evaluate our model; and\n",
    "- We used cross-validation to evaluate our model.\n",
    "\n",
    "Some things that we have not covered but may be interesting for the next exercise where you build your own model:\n",
    "- Different ways of tokenzing the texts\n",
    "- Hyperparamater tuning of MultinomialNB\n",
    "- Using a different classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8312f8-e3a4-4830-8e57-16aefc37a96b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "864fe63ee5e483ebd7ec5811b6d0d1f3",
     "grade": false,
     "grade_id": "E3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Best model (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b64cd-fd34-4c15-943a-654f34e35642",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f662f55fe791f7a778fc12b7a80a590",
     "grade": false,
     "grade_id": "E3b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The final goal is to implement any model of your choice. \n",
    "- You are free to choose any model, including random forests, support vector machines, etc. \n",
    "- Your model will be evaluated on a hidden test that is not included in the given data set, but shares the same characteristics. Therefore, if you want to obtain a high score on this hidden test set, you have to make sure to follow the best practices in validating your model (i.e., train, validation and test set) to prevent underfitting and overfitting.\n",
    "- Your score is determined based on the F1 score on the hidden test set. To get non-zero points, your model should perform better than the simple model we created above. To get the maximum number of points, your model should get a F1-score of at least **97.5%** on the hidden test.\n",
    "- The training of your model should take no longer than **45 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "80e5bcfc-76dc-4452-98ae-a1a935b1a1a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54ecf3fe07da6c230a9e5fb155f3354d",
     "grade": false,
     "grade_id": "E3b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def best_spamham(X, y):\n",
    "    \"\"\"\n",
    "    Implement your best performing model. Return a fitted model.\n",
    "    \"\"\"\n",
    "    model = make_pipeline(TfidfVectorizer(), LogisticRegression(C=9, solver='saga', penalty = 'l2'))\n",
    "    \n",
    "    return model.fit(X,y)\n",
    "\n",
    "# Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
    "#                 ('nb', MultinomialNB(alpha=0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e3b1d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 459, in _check_solver\n",
      "    solver\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1473, in fit\n",
      "    % self.l1_ratio\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\PC\\Python\\envs\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.97920406        nan 0.97906535 0.98754753 0.98754753\n",
      " 0.98754753 0.98754753        nan        nan        nan        nan\n",
      "        nan 0.98433088        nan 0.98446419 0.99008768 0.99008768\n",
      " 0.99008768 0.99008768        nan        nan        nan        nan\n",
      "        nan 0.98577391        nan 0.98600268 0.99095632 0.99095632\n",
      " 0.99095632 0.99104661        nan        nan        nan        nan\n",
      "        nan 0.98654883        nan 0.98681946 0.99127192 0.99127192\n",
      " 0.99127192 0.99122777        nan        nan        nan        nan\n",
      "        nan 0.98745848        nan 0.9880826  0.99145395 0.99145395\n",
      " 0.99145395 0.99145395        nan        nan        nan        nan\n",
      "        nan 0.98822659        nan 0.98885523 0.99163484 0.99163484\n",
      " 0.99163484 0.99159028        nan        nan        nan        nan\n",
      "        nan 0.98872706        nan 0.9892172  0.99195365 0.99195365\n",
      " 0.99195365 0.99195365        nan        nan        nan        nan\n",
      "        nan 0.98886521        nan 0.98936361 0.9920447  0.9920447\n",
      " 0.9920447  0.99199951        nan        nan        nan        nan\n",
      "        nan 0.98882796        nan 0.98954503 0.99222571 0.99222571\n",
      " 0.99222571 0.99227075        nan        nan        nan        nan\n",
      "        nan 0.98873842        nan 0.99018564 0.99222571 0.99227075\n",
      " 0.99222571 0.99222571        nan        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('logis', LogisticRegression(C=9.0, solver='saga'))])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "a = np.linspace(0.1,1,10)\n",
    "C = np.linspace(1,10,10)\n",
    "\n",
    "spamham_pipe = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                       ('logis', LogisticRegression())])\n",
    "spamham_param = {'logis__C':C,\n",
    "                'logis__penalty': ['l1','l2','elasticnet'],\n",
    "                'logis__solver': ['newton-cg', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "tune = GridSearchCV(spamham_pipe,spamham_param,cv=kf, scoring=make_scorer(f1_score, pos_label = 'spam'))\n",
    "\n",
    "tune.fit(X,y).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8c971432-ca77-4026-ab9b-823417d80ebf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b114afcba0478526791c0e9ea47c616",
     "grade": true,
     "grade_id": "E3b-test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model took 7.311 seconds to be trained.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9948557369715947"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No test available; the code shows how your model will be evaluated\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "best = best_spamham(X_train, y_train)\n",
    "predicted = best.predict(X_test)\n",
    "t1 = time.perf_counter()\n",
    "print(f'Your model took {t1-t0:.4} seconds to be trained.\\n')\n",
    "\n",
    "# The F1 score will be used to evaluate your score\n",
    "f1_score(y_test, predicted, pos_label='spam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e079c26-81d0-4b8b-9dc2-5316ce470bb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a107b1ab9e9ebd2c477d4aef1eb38634",
     "grade": false,
     "grade_id": "E4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Exercise 4: Regression on Californian house prices (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9b2d0-0d16-42d5-865c-715bc309e75f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb6fc5cddf81229bd5703666b13976cf",
     "grade": false,
     "grade_id": "E4-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The goal of this exercise to apply regression to predict house prices in California. Here is a brief description of the data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b40dee-ed28-46d6-9348-487d3967007b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d314792d3a007ec3973e49f3533771c",
     "grade": false,
     "grade_id": "E4-description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "```\n",
    "California Housing dataset\n",
    "--------------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
    "\n",
    "    :Attribute Information:\n",
    "        - MedInc        median income in block group\n",
    "        - HouseAge      median house age in block group\n",
    "        - AveRooms      average number of rooms per household\n",
    "        - AveBedrms     average number of bedrooms per household\n",
    "        - Population    block group population\n",
    "        - AveOccup      average number of household members\n",
    "        - Latitude      block group latitude\n",
    "        - Longitude     block group longitude\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "This dataset was obtained from the StatLib repository.\n",
    "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "The target variable is the median house value for California districts,\n",
    "expressed in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census\n",
    "block group. A block group is the smallest geographical unit for which the U.S.\n",
    "Census Bureau publishes sample data (a block group typically has a population\n",
    "of 600 to 3,000 people).\n",
    "\n",
    "An household is a group of people residing within a home. Since the average\n",
    "number of rooms and bedrooms in this dataset are provided per household, these\n",
    "columns may take surpinsingly large values for block groups with few households\n",
    "and many empty houses, such as vacation resorts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0849577-4bf2-4e5d-9da8-a3a516f880ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2a10dff160f41be6f48adec9f7451fc",
     "grade": false,
     "grade_id": "E4-text2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is the dataframe of the housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f6f066cc-c928-4cbd-b587-c5b8a7483667",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9275881ab7493bec9a979b52dd1160ca",
     "grade": false,
     "grade_id": "E4-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13646</th>\n",
       "      <td>1.9234</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.821023</td>\n",
       "      <td>1.099432</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>3.355114</td>\n",
       "      <td>34.08</td>\n",
       "      <td>-117.31</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3.2250</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.326640</td>\n",
       "      <td>1.117805</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>1.838019</td>\n",
       "      <td>37.81</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18240</th>\n",
       "      <td>2.5208</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.908163</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>448.0</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>37.40</td>\n",
       "      <td>-122.08</td>\n",
       "      <td>3.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12061</th>\n",
       "      <td>5.1929</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.424460</td>\n",
       "      <td>1.032374</td>\n",
       "      <td>939.0</td>\n",
       "      <td>3.377698</td>\n",
       "      <td>33.87</td>\n",
       "      <td>-117.57</td>\n",
       "      <td>1.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14177</th>\n",
       "      <td>3.0446</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.895075</td>\n",
       "      <td>1.002141</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>3.728051</td>\n",
       "      <td>32.72</td>\n",
       "      <td>-117.08</td>\n",
       "      <td>1.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "13646  1.9234      43.0  4.821023   1.099432      1181.0  3.355114     34.08   \n",
       "163    3.2250      46.0  4.326640   1.117805      1373.0  1.838019     37.81   \n",
       "18240  2.5208      52.0  3.908163   1.035714       448.0  2.285714     37.40   \n",
       "12061  5.1929      27.0  6.424460   1.032374       939.0  3.377698     33.87   \n",
       "14177  3.0446      32.0  4.895075   1.002141      1741.0  3.728051     32.72   \n",
       "\n",
       "       Longitude  MedHouseVal  \n",
       "13646    -117.31        0.746  \n",
       "163      -122.25        2.188  \n",
       "18240    -122.08        3.167  \n",
       "12061    -117.57        1.650  \n",
       "14177    -117.08        1.019  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv('data/house_prices.csv', index_col=0)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7ce95-8af1-4ad4-ad49-6e4506790a8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07db91f2f6f820577b75dc97903c8c53",
     "grade": false,
     "grade_id": "E4-text3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`X` contains the features and `y` contains the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "900cc39c-f4b5-4fc0-9352-92a684342877",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69efe07e6618a40bfbbc9b2254932cb2",
     "grade": false,
     "grade_id": "E4-import2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = housing.drop(['MedHouseVal'], axis=1).values\n",
    "y = housing['MedHouseVal'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111b639-7dab-467b-b380-9c46b1b973f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37b54e2b40874f8f77c1215fea8a5c7a",
     "grade": false,
     "grade_id": "E4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warm-up questions (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced1b0d-119e-406b-82bc-6c071b7a28b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b98a4975a8141168955a962b4dba5cf",
     "grade": false,
     "grade_id": "E4a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following five exercises are warm-up exercises to get you started with building a simple linear regression model. This simple model will sometimes produce questionable results (e.g., negative house prices), but you can ignore that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5cdf4-49c4-414b-a13e-d0bd84810cfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eec34e9a202292160245abda5f36cec8",
     "grade": false,
     "grade_id": "E4a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Naive Linear Regression (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda3302-d7e1-47b3-bea4-a0adde2cb13e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84958d6225a3cafad93e7505bee83cd3",
     "grade": false,
     "grade_id": "E4a1-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a first warm-up, fit a linear model to describe the relationship between the housing price and all available features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fde87d5e-d27c-4165-89c0-da81dc45ed59",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "071846b641410da8d2e3a76bc59e4094",
     "grade": false,
     "grade_id": "E4a1-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nlr_prices(X, y):\n",
    "    \"\"\"\n",
    "    Return a fitted a naive Linear Regression model using the input features directly.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "03ab39cc-1608-4ddc-bf19-2e8bcab9f1e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1222c51aa6a2615fc6cf2358aede7b91",
     "grade": true,
     "grade_id": "E4a1-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0933058685091055\n"
     ]
    }
   ],
   "source": [
    "nlr = nlr_prices(X, y)\n",
    "predicted = nlr.predict(X)\n",
    "result = predicted[0]\n",
    "print(result)\n",
    "\n",
    "assert_almost_equal(result, 1.093, 3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc1cc6-4ea4-4056-9c85-b1dcaaecd9eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "424d0f759681bb5a916ff92643077a76",
     "grade": false,
     "grade_id": "E4a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Metrics: Root Mean Squared Error (3 points)\n",
    "We'll use the [root mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) as metric for predicting the house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "25aa2fb5-f4f3-4b63-85fb-d2c4b5f4381f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7387c79a0b03da91e2fe2371e25bc490",
     "grade": false,
     "grade_id": "E4a2-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def rmse(target, predicted):\n",
    "    \"\"\"\n",
    "    Return the root mean squared error between the target and predicted vector.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    return mean_squared_error(target, predicted, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4df90dd8-5e3d-41c7-80e0-f7d90aedb833",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f984c3160d43e0aecae073b581ffa041",
     "grade": true,
     "grade_id": "E4a2-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7273901601185822\n"
     ]
    }
   ],
   "source": [
    "# Introduce train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "nlr = nlr_prices(X_train, y_train)\n",
    "\n",
    "predicted = nlr.predict(X_test)\n",
    "result = rmse(y_test, predicted)\n",
    "print(\"RMSE:\", result)\n",
    "\n",
    "assert_almost_equal(result, 0.727, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971fcee-781c-4a8d-8063-add457007bdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04880685b66173c2372efcf2528bed9f",
     "grade": false,
     "grade_id": "E4a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Adding polynomial features (3 points)\n",
    "We can often get even higher accuracy by simply adding more features. Let's add some polynomial features and also make a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fde0b429-cb5a-4115-95fe-8007895f153e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3cd325302530b96362413811d9ace2b",
     "grade": false,
     "grade_id": "E4a3-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plr_prices(X, y, d=2):\n",
    "    \"\"\"\n",
    "    Return a fitted linear regression model with polynomial features, where d is an input integer\n",
    "    denoting the degree of the polynomials. Use sklearn.pipeline.make_pipeline to create a pipeline.\n",
    "    \"\"\"\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    poly_model = make_pipeline(PolynomialFeatures(d,include_bias=False),\n",
    "                           LinearRegression(fit_intercept=True))\n",
    "    return poly_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4d78a1b7-e7cf-4d55-ba8e-7ba5dca97f66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d27e30e12e15608324a3987f9b06ec7a",
     "grade": true,
     "grade_id": "E4a3-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.8844998713604615\n"
     ]
    }
   ],
   "source": [
    "plr = plr_prices(X_train, y_train, 2)\n",
    "\n",
    "predicted = plr.predict(X_test)\n",
    "result = rmse(y_test, predicted)\n",
    "print(\"RMSE:\", result)\n",
    "\n",
    "assert_almost_equal(result, 1.884, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbbf52-a68b-4943-9c02-430fb2e6e898",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97ed212283c60d6c66285a9419f8259d",
     "grade": false,
     "grade_id": "E4a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ridge (3 points)\n",
    "The added polynomial features did not improve the model at all! Another step can often improve the model is to apply regularization, which constraints the magnitude of coefficients are constrained. For example, let's take a quick look at the coefficients obtained for our previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "828f842a-4b5a-499d-b984-71aa9c532b69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f457bac80befefa3db391fa2f0afc8d",
     "grade": false,
     "grade_id": "E4a4-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.377836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.689527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-32.293341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.015442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.050075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.900398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  44.000000\n",
       "mean   -0.377836\n",
       "std     5.689527\n",
       "min   -32.293341\n",
       "25%    -0.015442\n",
       "50%     0.000057\n",
       "75%     0.050075\n",
       "max     8.900398"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(plr.named_steps['linearregression'].coef_).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b20e3-5701-4d87-af6f-b9007198a1d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f409e7b1f3844bc97f2b19158d534cb",
     "grade": false,
     "grade_id": "E4a4-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here it can be seen that the majority of the coefficients are valued in the range of 0.01-0.03, but there are some coefficients with several orders of magnitude (-30.95, 8.42) as well. This means that our model is likely to ovefit on data that have high numbers for the features corresponding to these high coefficients. We will use [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) in this exercise as regularization. Moreover, another issue that our data set has is that features have different units (e.g., population size vs. average bedrooms). It's best to re-scale the data so that all features are affected similarly by the regularization strength, this can be done using `StandardScaler`, which renormalized each feature column to have zero mean and 1 standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e44a40f5-2487-4259-884a-6721cbadf9d8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99a0b9a2758a5816871f0fdc9a9093a7",
     "grade": false,
     "grade_id": "E4a4-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ridge_prices(X, y, d=2, alpha=1):\n",
    "    \"\"\"\n",
    "    Return a fitted linear regression model with polynomial features and L2 regularization, a.k.a Ridge. \n",
    "    d and alpha are the degree of the polynomial functions and the regularization factor, respectively.\n",
    "    Add a StandardScaler to the pipeline after applying the PolynomialFeatures step.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    regu_scaled_model = make_pipeline(PolynomialFeatures(d,include_bias=False),\n",
    "                                      StandardScaler(),\n",
    "                                      Ridge(alpha))\n",
    "\n",
    "    return regu_scaled_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b25c4b40-33eb-41de-82dd-1fc41899525e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90b535cb2cbe6ba59672d2a116b210b4",
     "grade": true,
     "grade_id": "E4a4-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6721336086335997\n"
     ]
    }
   ],
   "source": [
    "ridge = ridge_prices(X_train, y_train, 2, 0.01)\n",
    "\n",
    "predicted = ridge.predict(X_test)\n",
    "result = rmse(y_test, predicted)\n",
    "print(\"RMSE:\", result)\n",
    "\n",
    "assert_almost_equal(result, 0.672, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a26ff56-8dfc-4167-913c-ad6cadd7cf0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcfa0be1e926830e45e216faf3b2c2b7",
     "grade": false,
     "grade_id": "E4a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fine tuning the regularization parameter (3 points)\n",
    "The final step in this warm-up exercise is to fine-tune the regularization parameter. We have used `alpha=0.01` as regularization parameter in the previous exercise by simple hand-tuning, but there may be other coefficients that lead to better predictions. Therefore, we should include a search of the hyperparamter `alpha`. When optimizing over hyperparameters, we need to include cross-validation as well to prevent overfitting. We'll make use of `GridSearchCV` (see lecture notebook `05.03: Hyperparameters and model validation`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a286caf5-4863-4a31-a933-2b9c1cb49a09",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4482e90805b2c73ab40d4f071654e621",
     "grade": false,
     "grade_id": "E4a5-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def best_ridge_prices(X, y, d=2, alphas=[0.1], cv=5):\n",
    "    \"\"\"\n",
    "    Return a fitted linear regression model with polynomial features, feature scaling, \n",
    "    and L2 regularization (Ridge). Use GridSearchCV for searching for the best parameter\n",
    "    (only for the regularization parameter). \n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    d: int\n",
    "        the degree of the polynomial features\n",
    "    alphas: list[float]\n",
    "        the alpha parameters to try out\n",
    "    cv: int\n",
    "        the number of cross-validation folds, which should be passed to GridSearchCV\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    pipe = Pipeline([('poly', PolynomialFeatures(d,include_bias=False)),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('ridge', Ridge())])\n",
    "    param = {'ridge__alpha':alphas}\n",
    "    \n",
    "    best_model = GridSearchCV(pipe,param, cv=cv)\n",
    "    \n",
    "    return best_model.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9a980eb4-0bfc-410b-be2d-a84e6e37db18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f133aae5411a6b6c27afeb388aabe681",
     "grade": true,
     "grade_id": "E4a5-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 35938.13663804626\n",
      "RMSE: 0.8460730735731928\n"
     ]
    }
   ],
   "source": [
    "best_ridge = best_ridge_prices(X_train, y_train, alphas=np.logspace(-1, 10, 100)).best_estimator_\n",
    "print(\"Best alpha:\", best_ridge.named_steps['ridge'].get_params()['alpha'])\n",
    "\n",
    "predicted = best_ridge.predict(X_test)\n",
    "result = rmse(y_test, predicted)\n",
    "print(\"RMSE:\", result)\n",
    "\n",
    "assert_almost_equal(result, 0.846, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdbfa6-a3b7-4829-a39d-5092a670f643",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d98291fce899f1544c0bbec255a66f7a",
     "grade": false,
     "grade_id": "E4a-wrapup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the `rmse` is quite a bit higher than before. This is because our first 4 steps did not include cross-validation. \n",
    "\n",
    "This concludes the warm-up exercise. In summary:\n",
    "- We created simple linear regression model;\n",
    "- We added polynomial features to the above;\n",
    "- We implemented a Ridge model and used cross-validation to find the best alpha parameter.\n",
    "\n",
    "Suggestions for the next exercise:\n",
    "- Different models, e.g., random forest regression, Lasso, etc.\n",
    "- Better feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ac3a7-8f1d-4ba2-b30a-6c2f3af6922d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "476368d9f8ed60cea26e1871935c64bf",
     "grade": false,
     "grade_id": "E4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Best model (20 points)\n",
    "The final goal is to implement any model of your choice. \n",
    "- You are free to choose any model for this exercise.\n",
    "- Your model will be evaluated on a hidden test that is not included in the given data set, but shares the same characteristics. Therefore, to ensure a high score on this hidden test set, you should make sure to follow the best practices in validating your model (i.e., train, validation and test set) to avoid underfitting and overfitting.\n",
    "- Your score is determined based on the RMSE score evaluated on the hidden test set. To get non-zero points, your model should perform better than the simple model we created above. To get the maximum number of points, your model should get an score of at least an RMSE of 0.6 on the hidden test.\n",
    "- The training of your model should take no longer than **45 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cbeb5c4e-61be-407b-bfc0-6c811898d73a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff13560fee51dbfcbb8e0e16b0a8e07f",
     "grade": false,
     "grade_id": "E4b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def best_model(X, y):\n",
    "    \"\"\"\n",
    "    Implement and return your best performing model.\n",
    "    \"\"\"\n",
    "    model = make_pipeline(PolynomialFeatures(degree = 2),\n",
    "                         StandardScaler(),\n",
    "                         RandomForestRegressor(n_estimators=30, max_depth=20))\n",
    "    \n",
    "    return model.fit(X,y)\n",
    "\n",
    "# Best parameter (CV score=-0.586):\n",
    "# {'mlp__alpha': 0.0026389473684210528}\n",
    "\n",
    "# RandomForest\n",
    "# Your model took 76.16 seconds to be trained.\n",
    "# RMSE: 0.5233090846756641\n",
    "\n",
    "# RandomForestRegressor(n_estimators=20, max_depth=10)\n",
    "# Your model took 10.77 seconds to be trained.\n",
    "# RMSE: 0.5487015226421296\n",
    "\n",
    "# RandomForestRegressor(n_estimators=20, max_depth=15)\n",
    "# Your model took 11.48 seconds to be trained.\n",
    "# RMSE: 0.5358478431879755\n",
    "\n",
    "# RandomForestRegressor(n_estimators=30, max_depth=20)\n",
    "# Your model took 22.85 seconds to be trained.\n",
    "# RMSE: 0.5225372999606611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8186dd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=-0.552):\n",
      "{'rfr__criterion': 'squared_error', 'rfr__max_depth': 15, 'rfr__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "pf = PolynomialFeatures(degree = 2)\n",
    "# Define a Standard Scaler to normalize inputs\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# set the tolerance to a large value to make the example faster\n",
    "rfr = RandomForestRegressor()\n",
    "pipe = Pipeline([(\"pf\", pf), (\"scaler\", scaler), (\"rfr\", rfr)])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    \"rfr__n_estimators\": np.linspace(1,20,20, dtype=int),\n",
    "    \"rfr__max_depth\": [5,10,15,20],\n",
    "    \"rfr__criterion\": ['squared_error']\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, scoring='neg_root_mean_squared_error', cv=3)\n",
    "search.fit(X_train,y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0744da65-5c0b-4c19-8479-7128a8725343",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b7051b552e26124fe24f6e19a9b302e",
     "grade": true,
     "grade_id": "E4b-test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model took 13.32 seconds to be trained.\n",
      "\n",
      "RMSE: 0.5229864254730847\n"
     ]
    }
   ],
   "source": [
    "# No test available; the code shows how your model will be evaluated\n",
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "best = best_model(X_train, y_train)\n",
    "predicted = best.predict(X_test)\n",
    "t1 = time.perf_counter()\n",
    "print(f'Your model took {t1-t0:.4} seconds to be trained.\\n')\n",
    "\n",
    "# The RMSE will be used to evaluate your score\n",
    "result = rmse(y_test, predicted)\n",
    "print(\"RMSE:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
